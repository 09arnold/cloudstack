<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8" /><title>8.2. Citrix XenServer Installation for CloudStack</title><link rel="stylesheet" type="text/css" href="Common_Content/css/default.css" /><link rel="stylesheet" media="print" href="Common_Content/css/print.css" type="text/css" /><meta name="generator" content="publican 2.8" /><meta name="package" content="Apache_CloudStack-Installation_Guide-4.0.1-incubating-en-US-1-" /><link rel="home" href="index.html" title="CloudStack Installation Guide" /><link rel="up" href="hypervisor-installation.html" title="Chapter 8. Hypervisor Installation" /><link rel="prev" href="hypervisor-kvm-install-flow.html" title="8.1. KVM Hypervisor Host Installation" /><link rel="next" href="vmware-install.html" title="8.3. VMware vSphere Installation and Configuration" /></head><body><p id="title"><a class="left" href="http://cloudstack.org"><img src="Common_Content/images/image_left.png" alt="Product Site" /></a><a class="right" href="http://docs.cloudstack.org"><img src="Common_Content/images/image_right.png" alt="Documentation Site" /></a></p><ul class="docnav"><li class="previous"><a accesskey="p" href="hypervisor-kvm-install-flow.html"><strong>Prev</strong></a></li><li class="next"><a accesskey="n" href="vmware-install.html"><strong>Next</strong></a></li></ul><div xml:lang="en-US" class="section" id="citrix-xenserver-installation" lang="en-US"><div class="titlepage"><div><div><h2 class="title" id="citrix-xenserver-installation">8.2. Citrix XenServer Installation for CloudStack</h2></div></div></div><div class="para">
		If you want to use the Citrix XenServer hypervisor to run guest virtual machines, install XenServer 6.0 or XenServer 6.0.2 on the host(s) in your cloud. For an initial installation, follow the steps below. If you have previously installed XenServer and want to upgrade to another version, see <a class="xref" href="citrix-xenserver-installation.html#xenserver-version-upgrading">Section 8.2.11, “Upgrading XenServer Versions”</a>.
	</div><div class="section" id="system-requirements-xenserver-hosts"><div class="titlepage"><div><div><h3 class="title" id="system-requirements-xenserver-hosts">8.2.1. System Requirements for XenServer Hosts</h3></div></div></div><div class="itemizedlist"><ul><li class="listitem"><div class="para">
					The host must be certified as compatible with one of the following. See the Citrix Hardware Compatibility Guide: <a href="http://hcl.xensource.com">http://hcl.xensource.com</a>
				</div><div class="itemizedlist"><ul><li class="listitem"><div class="para">
							XenServer 5.6 SP2
						</div></li><li class="listitem"><div class="para">
							XenServer 6.0
						</div></li><li class="listitem"><div class="para">
							XenServer 6.0.2
						</div></li></ul></div></li><li class="listitem"><div class="para">
					You must re-install Citrix XenServer if you are going to re-use a host from a previous install.
				</div></li><li class="listitem"><div class="para">
					Must support HVM (Intel-VT or AMD-V enabled)
				</div></li><li class="listitem"><div class="para">
					Be sure all the hotfixes provided by the hypervisor vendor are applied. Track the release of hypervisor patches through your hypervisor vendor’s support channel, and apply patches as soon as possible after they are released. CloudStack will not track or notify you of required hypervisor patches. It is essential that your hosts are completely up to date with the provided hypervisor patches. The hypervisor vendor is likely to refuse to support any system that is not up to date with patches.
				</div></li><li class="listitem"><div class="para">
					All hosts within a cluster must be homogenous. The CPUs must be of the same type, count, and feature flags.
				</div></li><li class="listitem"><div class="para">
					Must support HVM (Intel-VT or AMD-V enabled in BIOS)
				</div></li><li class="listitem"><div class="para">
					64-bit x86 CPU (more cores results in better performance)
				</div></li><li class="listitem"><div class="para">
					Hardware virtualization support required
				</div></li><li class="listitem"><div class="para">
					4 GB of memory
				</div></li><li class="listitem"><div class="para">
					36 GB of local disk
				</div></li><li class="listitem"><div class="para">
					At least 1 NIC
				</div></li><li class="listitem"><div class="para">
					Statically allocated IP Address
				</div></li><li class="listitem"><div class="para">
					When you deploy CloudStack, the hypervisor host must not have any VMs already running
				</div></li></ul></div><div class="warning"><div class="admonition_header"><h2>Warning</h2></div><div class="admonition"><div class="para">
				The lack of up-do-date hotfixes can lead to data corruption and lost VMs.
			</div></div></div></div><div class="section" id="xenserver-installation-steps"><div class="titlepage"><div><div><h3 class="title" id="xenserver-installation-steps">8.2.2. XenServer Installation Steps</h3></div></div></div><div class="orderedlist"><ol><li class="listitem"><div class="para">
					From <a href="https://www.citrix.com/English/ss/downloads/">https://www.citrix.com/English/ss/downloads/</a>, download the appropriate version of XenServer for your CloudStack version (see <a class="xref" href="citrix-xenserver-installation.html#system-requirements-xenserver-hosts">Section 8.2.1, “System Requirements for XenServer Hosts”</a>). Install it using the Citrix XenServer Installation Guide.
				</div></li><li class="listitem"><div class="para">
					After installation, perform the following configuration steps, which are described in the next few sections:
				</div><div class="informaltable"><table border="1"><colgroup><col align="left" class="c1" width="50%" /><col align="left" class="c2" width="50%" /></colgroup><thead><tr><th align="left">
									<div class="para">
										Required
									</div>
								</th><th align="left">
									<div class="para">
										Optional
									</div>
								</th></tr></thead><tbody><tr><td align="left">
									<div class="para">
										<a class="xref" href="citrix-xenserver-installation.html#config-xenserver-dom0-memory">Section 8.2.3, “Configure XenServer dom0 Memory”</a>
									</div>
								</td><td align="left">
									<div class="para">
										<a class="xref" href="citrix-xenserver-installation.html#xenserver-support-pkg-installation">Section 8.2.7, “Install CloudStack XenServer Support Package (CSP)”</a>
									</div>
								</td></tr><tr><td align="left">
									<div class="para">
										<a class="xref" href="citrix-xenserver-installation.html#xenserver-username-password">Section 8.2.4, “Username and Password”</a>
									</div>
								</td><td align="left">
									<div class="para">
										Set up SR if not using NFS, iSCSI, or local disk; see <a class="xref" href="citrix-xenserver-installation.html#xenserver-primary-storage-setup">Section 8.2.8, “Primary Storage Setup for XenServer”</a>
									</div>
								</td></tr><tr><td align="left">
									<div class="para">
										<a class="xref" href="citrix-xenserver-installation.html#xenserver-time-sync">Section 8.2.5, “Time Synchronization”</a>
									</div>
								</td><td align="left">
									<div class="para">
										<a class="xref" href="citrix-xenserver-installation.html#xenserver-iscsi-multipath-setup">Section 8.2.9, “iSCSI Multipath Setup for XenServer (Optional)”</a>
									</div>
								</td></tr><tr><td align="left">
									<div class="para">
										<a class="xref" href="citrix-xenserver-installation.html#xenserver-get-deploy-license">Section 8.2.6.1, “Getting and Deploying a License”</a>
									</div>
								</td><td align="left">
									<div class="para">
										<a class="xref" href="citrix-xenserver-installation.html#xenserver-physical-network-setup">Section 8.2.10, “Physical Networking Setup for XenServer”</a>
									</div>
								</td></tr></tbody></table></div></li></ol></div></div><div class="section" id="config-xenserver-dom0-memory"><div class="titlepage"><div><div><h3 class="title" id="config-xenserver-dom0-memory">8.2.3. Configure XenServer dom0 Memory</h3></div></div></div><div class="para">
			Configure the XenServer dom0 settings to allocate more memory to dom0. This can enable XenServer to handle larger numbers of virtual machines. We recommend 2940 MB of RAM for XenServer dom0. For instructions on how to do this, see <a href="http://support.citrix.com/article/CTX126531">http://support.citrix.com/article/CTX126531</a>. The article refers to XenServer 5.6, but the same information applies to XenServer 6.0.
		</div></div><div class="section" id="xenserver-username-password"><div class="titlepage"><div><div><h3 class="title" id="xenserver-username-password">8.2.4. Username and Password</h3></div></div></div><div class="para">
			All XenServers in a cluster must have the same username and password as configured in CloudStack.
		</div></div><div class="section" id="xenserver-time-sync"><div class="titlepage"><div><div><h3 class="title" id="xenserver-time-sync">8.2.5. Time Synchronization</h3></div></div></div><div class="para">
			The host must be set to use NTP. All hosts in a pod must have the same time.
		</div><div class="orderedlist"><ol><li class="listitem"><div class="para">
					Install NTP.
				</div><pre class="programlisting"># yum install ntp</pre></li><li class="listitem"><div class="para">
					Edit the NTP configuration file to point to your NTP server.
				</div><pre class="programlisting"># vi /etc/ntp.conf</pre><div class="para">
					Add one or more server lines in this file with the names of the NTP servers you want to use. For example:
				</div><pre class="programlisting">server 0.xenserver.pool.ntp.org
server 1.xenserver.pool.ntp.org
server 2.xenserver.pool.ntp.org
server 3.xenserver.pool.ntp.org
</pre></li><li class="listitem"><div class="para">
					Restart the NTP client.
				</div><pre class="programlisting"># service ntpd restart</pre></li><li class="listitem"><div class="para">
					Make sure NTP will start again upon reboot.
				</div><pre class="programlisting"># chkconfig ntpd on</pre></li></ol></div></div><div class="section" id="xenserver-licensing"><div class="titlepage"><div><div><h3 class="title" id="xenserver-licensing">8.2.6. Licensing</h3></div></div></div><div class="para">
			Citrix XenServer Free version provides 30 days usage without a license. Following the 30 day trial, XenServer requires a free activation and license. You can choose to install a license now or skip this step. If you skip this step, you will need to install a license when you activate and license the XenServer.
		</div><div class="section" id="xenserver-get-deploy-license"><div class="titlepage"><div><div><h4 class="title" id="xenserver-get-deploy-license">8.2.6.1. Getting and Deploying a License</h4></div></div></div><div class="para">
				If you choose to install a license now you will need to use the XenCenter to activate and get a license.
			</div><div class="orderedlist"><ol><li class="listitem"><div class="para">
						In XenCenter, click Tools &gt; License manager.
					</div></li><li class="listitem"><div class="para">
						Select your XenServer and select Activate Free XenServer.
					</div></li><li class="listitem"><div class="para">
						Request a license.
					</div></li></ol></div><div class="para">
				You can install the license with XenCenter or using the xe command line tool.
			</div></div></div><div class="section" id="xenserver-support-pkg-installation"><div class="titlepage"><div><div><h3 class="title" id="xenserver-support-pkg-installation">8.2.7. Install CloudStack XenServer Support Package (CSP)</h3></div></div></div><div class="para">
			(Optional)
		</div><div class="para">
			To enable security groups, elastic load balancing, and elastic IP on XenServer, download and install the CloudStack XenServer Support Package (CSP). After installing XenServer, perform the following additional steps on each XenServer host.
		</div><div class="orderedlist"><ol><li class="listitem"><div class="para">
					Download the CSP software onto the XenServer host from one of the following links:
				</div><div class="para">
					For XenServer 6.0.2:
				</div><div class="para">
					<a href="http://download.cloud.com/releases/3.0.1/XS-6.0.2/xenserver-cloud-supp.tgz">http://download.cloud.com/releases/3.0.1/XS-6.0.2/xenserver-cloud-supp.tgz</a>
				</div><div class="para">
					For XenServer 5.6 SP2:
				</div><div class="para">
					<a href="http://download.cloud.com/releases/2.2.0/xenserver-cloud-supp.tgz">http://download.cloud.com/releases/2.2.0/xenserver-cloud-supp.tgz</a>
				</div><div class="para">
					For XenServer 6.0:
				</div><div class="para">
					<a href="http://download.cloud.com/releases/3.0/xenserver-cloud-supp.tgz">http://download.cloud.com/releases/3.0/xenserver-cloud-supp.tgz</a>
				</div></li><li class="listitem"><div class="para">
					Extract the file:
				</div><pre class="programlisting"># tar xf xenserver-cloud-supp.tgz</pre></li><li class="listitem"><div class="para">
					Run the following script:
				</div><pre class="programlisting"># xe-install-supplemental-pack xenserver-cloud-supp.iso</pre></li><li class="listitem"><div class="para">
					If the XenServer host is part of a zone that uses basic networking, disable Open vSwitch (OVS):
				</div><pre class="programlisting"># xe-switch-network-backend  bridge</pre><div class="para">
					Restart the host machine when prompted.
				</div></li></ol></div><div class="para">
			The XenServer host is now ready to be added to CloudStack.
		</div></div><div class="section" id="xenserver-primary-storage-setup"><div class="titlepage"><div><div><h3 class="title" id="xenserver-primary-storage-setup">8.2.8. Primary Storage Setup for XenServer</h3></div></div></div><div class="para">
			CloudStack natively supports NFS, iSCSI and local storage. If you are using one of these storage types, there is no need to create the XenServer Storage Repository ("SR").
		</div><div class="para">
			If, however, you would like to use storage connected via some other technology, such as FiberChannel, you must set up the SR yourself. To do so, perform the following steps. If you have your hosts in a XenServer pool, perform the steps on the master node. If you are working with a single XenServer which is not part of a cluster, perform the steps on that XenServer.
		</div><div class="orderedlist"><ol><li class="listitem"><div class="para">
					Connect FiberChannel cable to all hosts in the cluster and to the FiberChannel storage host.
				</div></li><li class="listitem"><div class="para">
					Rescan the SCSI bus. Either use the following command or use XenCenter to perform an HBA rescan.
				</div><pre class="programlisting"># scsi-rescan</pre></li><li class="listitem"><div class="para">
					Repeat step 2 on every host.
				</div></li><li class="listitem"><div class="para">
					Check to be sure you see the new SCSI disk.
				</div><pre class="programlisting"># ls /dev/disk/by-id/scsi-360a98000503365344e6f6177615a516b -l</pre><div class="para">
					The output should look like this, although the specific file name will be different (scsi-&lt;scsiID&gt;):
				</div><pre class="programlisting">lrwxrwxrwx 1 root root 9 Mar 16 13:47
/dev/disk/by-id/scsi-360a98000503365344e6f6177615a516b -&gt; ../../sdc
</pre></li><li class="listitem"><div class="para">
					Repeat step 4 on every host.
				</div></li><li class="listitem"><div class="para">
					On the storage server, run this command to get a unique ID for the new SR.
				</div><pre class="programlisting"># uuidgen</pre><div class="para">
					The output should look like this, although the specific ID will be different:
				</div><pre class="programlisting">e6849e96-86c3-4f2c-8fcc-350cc711be3d</pre></li><li class="listitem"><div class="para">
					Create the FiberChannel SR. In name-label, use the unique ID you just generated.
				</div><pre class="programlisting">
# xe sr-create type=lvmohba shared=true
device-config:SCSIid=360a98000503365344e6f6177615a516b
name-label="e6849e96-86c3-4f2c-8fcc-350cc711be3d"
</pre><div class="para">
					This command returns a unique ID for the SR, like the following example (your ID will be different):
				</div><pre class="programlisting">7a143820-e893-6c6a-236e-472da6ee66bf</pre></li><li class="listitem"><div class="para">
					To create a human-readable description for the SR, use the following command. In uuid, use the SR ID returned by the previous command. In name-description, set whatever friendly text you prefer.
				</div><pre class="programlisting"># xe sr-param-set uuid=7a143820-e893-6c6a-236e-472da6ee66bf name-description="Fiber Channel storage repository"</pre><div class="para">
					Make note of the values you will need when you add this storage to CloudStack later (see <a class="xref" href="primary-storage-add.html">Section 6.6, “Add Primary Storage”</a>). In the Add Primary Storage dialog, in Protocol, you will choose PreSetup. In SR Name-Label, you will enter the name-label you set earlier (in this example, e6849e96-86c3-4f2c-8fcc-350cc711be3d).
				</div></li><li class="listitem"><div class="para">
					(Optional) If you want to enable multipath I/O on a FiberChannel SAN, refer to the documentation provided by the SAN vendor.
				</div></li></ol></div></div><div class="section" id="xenserver-iscsi-multipath-setup"><div class="titlepage"><div><div><h3 class="title" id="xenserver-iscsi-multipath-setup">8.2.9. iSCSI Multipath Setup for XenServer (Optional)</h3></div></div></div><div class="para">
			When setting up the storage repository on a Citrix XenServer, you can enable multipath I/O, which uses redundant physical components to provide greater reliability in the connection between the server and the SAN. To enable multipathing, use a SAN solution that is supported for Citrix servers and follow the procedures in Citrix documentation. The following links provide a starting point:
		</div><div class="itemizedlist"><ul><li class="listitem"><div class="para">
					<a href="http://support.citrix.com/article/CTX118791">http://support.citrix.com/article/CTX118791</a>
				</div></li><li class="listitem"><div class="para">
					<a href="http://support.citrix.com/article/CTX125403">http://support.citrix.com/article/CTX125403</a>
				</div></li></ul></div><div class="para">
			You can also ask your SAN vendor for advice about setting up your Citrix repository for multipathing.
		</div><div class="para">
			Make note of the values you will need when you add this storage to the CloudStack later (see <a class="xref" href="primary-storage-add.html">Section 6.6, “Add Primary Storage”</a>). In the Add Primary Storage dialog, in Protocol, you will choose PreSetup. In SR Name-Label, you will enter the same name used to create the SR.
		</div><div class="para">
			If you encounter difficulty, address the support team for the SAN provided by your vendor. If they are not able to solve your issue, see Contacting Support.
		</div></div><div class="section" id="xenserver-physical-network-setup"><div class="titlepage"><div><div><h3 class="title" id="xenserver-physical-network-setup">8.2.10. Physical Networking Setup for XenServer</h3></div></div></div><div class="para">
			Once XenServer has been installed, you may need to do some additional network configuration. At this point in the installation, you should have a plan for what NICs the host will have and what traffic each NIC will carry. The NICs should be cabled as necessary to implement your plan.
		</div><div class="para">
			If you plan on using NIC bonding, the NICs on all hosts in the cluster must be cabled exactly the same. For example, if eth0 is in the private bond on one host in a cluster, then eth0 must be in the private bond on all hosts in the cluster.
		</div><div class="para">
			The IP address assigned for the management network interface must be static. It can be set on the host itself or obtained via static DHCP.
		</div><div class="para">
			CloudStack configures network traffic of various types to use different NICs or bonds on the XenServer host. You can control this process and provide input to the Management Server through the use of XenServer network name labels. The name labels are placed on physical interfaces or bonds and configured in CloudStack. In some simple cases the name labels are not required.
		</div><div class="section" id="xenserver-public-network-config"><div class="titlepage"><div><div><h4 class="title" id="xenserver-public-network-config">8.2.10.1. Configuring Public Network with a Dedicated NIC for XenServer (Optional)</h4></div></div></div><div class="para">
				CloudStack supports the use of a second NIC (or bonded pair of NICs, described in <a class="xref" href="citrix-xenserver-installation.html#xenserver-nic-bonding">Section 8.2.10.4, “NIC Bonding for XenServer (Optional)”</a>) for the public network. If bonding is not used, the public network can be on any NIC and can be on different NICs on the hosts in a cluster. For example, the public network can be on eth0 on node A and eth1 on node B. However, the XenServer name-label for the public network must be identical across all hosts. The following examples set the network label to "cloud-public". After the management server is installed and running you must configure it with the name of the chosen network label (e.g. "cloud-public"); this is discussed in <a class="xref" href="management-server-install-flow.html">Section 4.5, “Management Server Installation”</a>.
			</div><div class="para">
				If you are using two NICs bonded together to create a public network, see <a class="xref" href="citrix-xenserver-installation.html#xenserver-nic-bonding">Section 8.2.10.4, “NIC Bonding for XenServer (Optional)”</a>.
			</div><div class="para">
				If you are using a single dedicated NIC to provide public network access, follow this procedure on each new host that is added to CloudStack before adding the host.
			</div><div class="orderedlist"><ol><li class="listitem"><div class="para">
						Run xe network-list and find the public network. This is usually attached to the NIC that is public. Once you find the network make note of its UUID. Call this &lt;UUID-Public&gt;.
					</div></li><li class="listitem"><div class="para">
						Run the following command.
					</div><pre class="programlisting"># xe network-param-set name-label=cloud-public uuid=&lt;UUID-Public&gt;</pre></li></ol></div></div><div class="section" id="xenserver-multi-guest-network-config"><div class="titlepage"><div><div><h4 class="title" id="xenserver-multi-guest-network-config">8.2.10.2. Configuring Multiple Guest Networks for XenServer (Optional)</h4></div></div></div><div class="para">
				CloudStack supports the use of multiple guest networks with the XenServer hypervisor. Each network is assigned a name-label in XenServer. For example, you might have two networks with the labels "cloud-guest" and "cloud-guest2". After the management server is installed and running, you must add the networks and use these labels so that CloudStack is aware of the networks.
			</div><div class="para">
				Follow this procedure on each new host before adding the host to CloudStack:
			</div><div class="orderedlist"><ol><li class="listitem"><div class="para">
						Run xe network-list and find one of the guest networks. Once you find the network make note of its UUID. Call this &lt;UUID-Guest&gt;.
					</div></li><li class="listitem"><div class="para">
						Run the following command, substituting your own name-label and uuid values.
					</div><pre class="programlisting"># xe network-param-set name-label=&lt;cloud-guestN&gt; uuid=&lt;UUID-Guest&gt;</pre></li><li class="listitem"><div class="para">
						Repeat these steps for each additional guest network, using a different name-label and uuid each time.
					</div></li></ol></div></div><div class="section" id="xenserver-separate-storage-network"><div class="titlepage"><div><div><h4 class="title" id="xenserver-separate-storage-network">8.2.10.3. Separate Storage Network for XenServer (Optional)</h4></div></div></div><div class="para">
				You can optionally set up a separate storage network. This should be done first on the host, before implementing the bonding steps below. This can be done using one or two available NICs. With two NICs bonding may be done as above. It is the administrator's responsibility to set up a separate storage network.
			</div><div class="para">
				Give the storage network a different name-label than what will be given for other networks.
			</div><div class="para">
				For the separate storage network to work correctly, it must be the only interface that can ping the primary storage device's IP address. For example, if eth0 is the management network NIC, ping -I eth0 &lt;primary storage device IP&gt; must fail. In all deployments, secondary storage devices must be pingable from the management network NIC or bond. If a secondary storage device has been placed on the storage network, it must also be pingable via the storage network NIC or bond on the hosts as well.
			</div><div class="para">
				You can set up two separate storage networks as well. For example, if you intend to implement iSCSI multipath, dedicate two non-bonded NICs to multipath. Each of the two networks needs a unique name-label.
			</div><div class="para">
				If no bonding is done, the administrator must set up and name-label the separate storage network on all hosts (masters and slaves).
			</div><div class="para">
				Here is an example to set up eth5 to access a storage network on 172.16.0.0/24.
			</div><pre class="programlisting">
# xe pif-list host-name-label='hostname' device=eth5
uuid(RO): ab0d3dd4-5744-8fae-9693-a022c7a3471d
device ( RO): eth5
#xe pif-reconfigure-ip DNS=172.16.3.3 gateway=172.16.0.1 IP=172.16.0.55 mode=static netmask=255.255.255.0 uuid=ab0d3dd4-5744-8fae-9693-a022c7a3471d</pre></div><div class="section" id="xenserver-nic-bonding"><div class="titlepage"><div><div><h4 class="title" id="xenserver-nic-bonding">8.2.10.4. NIC Bonding for XenServer (Optional)</h4></div></div></div><div class="para">
				XenServer supports Source Level Balancing (SLB) NIC bonding. Two NICs can be bonded together to carry public, private, and guest traffic, or some combination of these. Separate storage networks are also possible. Here are some example supported configurations:
			</div><div class="itemizedlist"><ul><li class="listitem"><div class="para">
						2 NICs on private, 2 NICs on public, 2 NICs on storage
					</div></li><li class="listitem"><div class="para">
						2 NICs on private, 1 NIC on public, storage uses management network
					</div></li><li class="listitem"><div class="para">
						2 NICs on private, 2 NICs on public, storage uses management network
					</div></li><li class="listitem"><div class="para">
						1 NIC for private, public, and storage
					</div></li></ul></div><div class="para">
				All NIC bonding is optional.
			</div><div class="para">
				XenServer expects all nodes in a cluster will have the same network cabling and same bonds implemented. In an installation the master will be the first host that was added to the cluster and the slave hosts will be all subsequent hosts added to the cluster. The bonds present on the master set the expectation for hosts added to the cluster later. The procedure to set up bonds on the master and slaves are different, and are described below. There are several important implications of this:
			</div><div class="itemizedlist"><ul><li class="listitem"><div class="para">
						You must set bonds on the first host added to a cluster. Then you must use xe commands as below to establish the same bonds in the second and subsequent hosts added to a cluster.
					</div></li><li class="listitem"><div class="para">
						Slave hosts in a cluster must be cabled exactly the same as the master. For example, if eth0 is in the private bond on the master, it must be in the management network for added slave hosts.
					</div></li></ul></div><div class="section" id="management-network-bonding"><div class="titlepage"><div><div><h5 class="title" id="management-network-bonding">8.2.10.4.1. Management Network Bonding</h5></div></div></div><div class="para">
					The administrator must bond the management network NICs prior to adding the host to CloudStack.
				</div></div><div class="section" id="first-host-private-bond"><div class="titlepage"><div><div><h5 class="title" id="first-host-private-bond">8.2.10.4.2. Creating a Private Bond on the First Host in the Cluster</h5></div></div></div><div class="para">
					Use the following steps to create a bond in XenServer. These steps should be run on only the first host in a cluster. This example creates the cloud-private network with two physical NICs (eth0 and eth1) bonded into it.
				</div><div class="orderedlist"><ol><li class="listitem"><div class="para">
							Find the physical NICs that you want to bond together.
						</div><pre class="programlisting"># xe pif-list host-name-label='hostname' device=eth0
# xe pif-list host-name-label='hostname' device=eth1</pre><div class="para">
							These command shows the eth0 and eth1 NICs and their UUIDs. Substitute the ethX devices of your choice. Call the UUID's returned by the above command slave1-UUID and slave2-UUID.
						</div></li><li class="listitem"><div class="para">
							Create a new network for the bond. For example, a new network with name "cloud-private".
						</div><div class="para">
							<span class="bold bold"><strong>This label is important. CloudStack looks for a network by a name you configure. You must use the same name-label for all hosts in the cloud for the management network.</strong></span>
						</div><pre class="programlisting"># xe network-create name-label=cloud-private
# xe bond-create network-uuid=[uuid of cloud-private created above]
pif-uuids=[slave1-uuid],[slave2-uuid]</pre></li></ol></div><div class="para">
					Now you have a bonded pair that can be recognized by CloudStack as the management network.
				</div></div><div class="section" id="public-network-bonding"><div class="titlepage"><div><div><h5 class="title" id="public-network-bonding">8.2.10.4.3. Public Network Bonding</h5></div></div></div><div class="para">
					Bonding can be implemented on a separate, public network. The administrator is responsible for creating a bond for the public network if that network will be bonded and will be separate from the management network.
				</div></div><div class="section" id="first-host-public-network-bond"><div class="titlepage"><div><div><h5 class="title" id="first-host-public-network-bond">8.2.10.4.4. Creating a Public Bond on the First Host in the Cluster</h5></div></div></div><div class="para">
					These steps should be run on only the first host in a cluster. This example creates the cloud-public network with two physical NICs (eth2 and eth3) bonded into it.
				</div><div class="orderedlist"><ol><li class="listitem"><div class="para">
							Find the physical NICs that you want to bond together.
						</div><pre class="programlisting">#xe pif-list host-name-label='hostname' device=eth2
# xe pif-list host-name-label='hostname' device=eth3</pre><div class="para">
							These command shows the eth2 and eth3 NICs and their UUIDs. Substitute the ethX devices of your choice. Call the UUID's returned by the above command slave1-UUID and slave2-UUID.
						</div></li><li class="listitem"><div class="para">
							Create a new network for the bond. For example, a new network with name "cloud-public".
						</div><div class="para">
							<span class="bold bold"><strong>This label is important. CloudStack looks for a network by a name you configure. You must use the same name-label for all hosts in the cloud for the public network.</strong></span>
						</div><pre class="programlisting"># xe network-create name-label=cloud-public
# xe bond-create network-uuid=[uuid of cloud-public created above]
pif-uuids=[slave1-uuid],[slave2-uuid]</pre></li></ol></div><div class="para">
					Now you have a bonded pair that can be recognized by CloudStack as the public network.
				</div></div><div class="section" id="adding-more-hosts"><div class="titlepage"><div><div><h5 class="title" id="adding-more-hosts">8.2.10.4.5. Adding More Hosts to the Cluster</h5></div></div></div><div class="para">
					With the bonds (if any) established on the master, you should add additional, slave hosts. Run the following command for all additional hosts to be added to the cluster. This will cause the host to join the master in a single XenServer pool.
				</div><pre class="programlisting"># xe pool-join master-address=[master IP] master-username=root
master-password=[your password]</pre></div><div class="section" id="complete-bonding-setup"><div class="titlepage"><div><div><h5 class="title" id="complete-bonding-setup">8.2.10.4.6. Complete the Bonding Setup Across the Cluster</h5></div></div></div><div class="para">
					With all hosts added to the pool, run the cloud-setup-bond script. This script will complete the configuration and set up of the bonds across all hosts in the cluster.
				</div><div class="orderedlist"><ol><li class="listitem"><div class="para">
							Copy the script from the Management Server in /usr/lib64/cloud/common/scripts/vm/hypervisor/xenserver/cloud-setup-bonding.sh to the master host and ensure it is executable.
						</div></li><li class="listitem"><div class="para">
							Run the script:
						</div><pre class="programlisting"># ./cloud-setup-bonding.sh</pre></li></ol></div><div class="para">
					Now the bonds are set up and configured properly across the cluster.
				</div></div></div></div><div class="section" id="xenserver-version-upgrading"><div class="titlepage"><div><div><h3 class="title" id="xenserver-version-upgrading">8.2.11. Upgrading XenServer Versions</h3></div></div></div><div class="para">
			This section tells how to upgrade XenServer software on CloudStack hosts. The actual upgrade is described in XenServer documentation, but there are some additional steps you must perform before and after the upgrade.
		</div><div class="note"><div class="admonition_header"><h2>Note</h2></div><div class="admonition"><div class="para">
				Be sure the hardware is certified compatible with the new version of XenServer.
			</div></div></div><div class="para">
			To upgrade XenServer:
		</div><div class="orderedlist"><ol><li class="listitem"><div class="para">
					Upgrade the database. On the Management Server node:
				</div><div class="orderedlist"><ol class="loweralpha"><li class="listitem"><div class="para">
							Back up the database:
						</div><pre class="programlisting"># mysqldump --user=root --databases cloud &gt; cloud.backup.sql
# mysqldump --user=root --databases cloud_usage &gt; cloud_usage.backup.sql</pre></li><li class="listitem"><div class="para">
							You might need to change the OS type settings for VMs running on the upgraded hosts.
						</div><div class="itemizedlist"><ul><li class="listitem"><div class="para">
									If you upgraded from XenServer 5.6 GA to XenServer 5.6 SP2, change any VMs that have the OS type CentOS 5.5 (32-bit), Oracle Enterprise Linux 5.5 (32-bit), or Red Hat Enterprise Linux 5.5 (32-bit) to Other Linux (32-bit). Change any VMs that have the 64-bit versions of these same OS types to Other Linux (64-bit).
								</div></li><li class="listitem"><div class="para">
									If you upgraded from XenServer 5.6 SP2 to XenServer 6.0.2, change any VMs that have the OS type CentOS 5.6 (32-bit), CentOS 5.7 (32-bit), Oracle Enterprise Linux 5.6 (32-bit), Oracle Enterprise Linux 5.7 (32-bit), Red Hat Enterprise Linux 5.6 (32-bit) , or Red Hat Enterprise Linux 5.7 (32-bit) to Other Linux (32-bit). Change any VMs that have the 64-bit versions of these same OS types to Other Linux (64-bit).
								</div></li><li class="listitem"><div class="para">
									If you upgraded from XenServer 5.6 to XenServer 6.0.2, do all of the above.
								</div></li></ul></div></li><li class="listitem"><div class="para">
							Restart the Management Server and Usage Server. You only need to do this once for all clusters.
						</div><pre class="programlisting"># service cloud-management start
# service cloud-usage start</pre></li></ol></div></li><li class="listitem"><div class="para">
					Disconnect the XenServer cluster from CloudStack.
				</div><div class="orderedlist"><ol class="loweralpha"><li class="listitem"><div class="para">
							Log in to the CloudStack UI as root.
						</div></li><li class="listitem"><div class="para">
							Navigate to the XenServer cluster, and click Actions – Unmanage.
						</div></li><li class="listitem"><div class="para">
							Watch the cluster status until it shows Unmanaged.
						</div></li></ol></div></li><li class="listitem"><div class="para">
					Log in to one of the hosts in the cluster, and run this command to clean up the VLAN:
				</div><pre class="programlisting"># . /opt/xensource/bin/cloud-clean-vlan.sh</pre></li><li class="listitem"><div class="para">
					Still logged in to the host, run the upgrade preparation script:
				</div><pre class="programlisting"># /opt/xensource/bin/cloud-prepare-upgrade.sh</pre><div class="para">
					Troubleshooting: If you see the error "can't eject CD," log in to the VM and umount the CD, then run the script again.
				</div></li><li class="listitem"><div class="para">
					Upgrade the XenServer software on all hosts in the cluster. Upgrade the master first.
				</div><div class="orderedlist"><ol class="loweralpha"><li class="listitem"><div class="para">
							Live migrate all VMs on this host to other hosts. See the instructions for live migration in the Administrator's Guide.
						</div><div class="para">
							Troubleshooting: You might see the following error when you migrate a VM:
						</div><pre class="programlisting">[root@xenserver-qa-2-49-4 ~]# xe vm-migrate live=true host=xenserver-qa-2-49-5 vm=i-2-8-VM
You attempted an operation on a VM which requires PV drivers to be installed but the drivers were not detected.
vm: b6cf79c8-02ee-050b-922f-49583d9f1a14 (i-2-8-VM)</pre><div class="para">
							To solve this issue, run the following:
						</div><pre class="programlisting"># /opt/xensource/bin/make_migratable.sh  b6cf79c8-02ee-050b-922f-49583d9f1a14</pre></li><li class="listitem"><div class="para">
							Reboot the host.
						</div></li><li class="listitem"><div class="para">
							Upgrade to the newer version of XenServer. Use the steps in XenServer documentation.
						</div></li><li class="listitem"><div class="para">
							After the upgrade is complete, copy the following files from the management server to this host, in the directory locations shown below:
						</div><div class="informaltable"><table border="1"><colgroup><col align="left" class="c1" width="50%" /><col align="left" class="c2" width="50%" /></colgroup><thead><tr><th align="left">
											<div class="para">
												Copy this Management Server file...
											</div>
										</th><th align="left">
											<div class="para">
												...to this location on the XenServer host
											</div>
										</th></tr></thead><tbody><tr><td align="left">
											<div class="para">
												/usr/lib64/cloud/common/scripts/vm/hypervisor/xenserver/xenserver60/NFSSR.py
											</div>
										</td><td align="left">
											<div class="para">
												/opt/xensource/sm/NFSSR.py
											</div>
										</td></tr><tr><td align="left">
											<div class="para">
												/usr/lib64/cloud/common/scripts/vm/hypervisor/xenserver/setupxenserver.sh
											</div>
										</td><td align="left">
											<div class="para">
												/opt/xensource/bin/setupxenserver.sh
											</div>
										</td></tr><tr><td align="left">
											<div class="para">
												/usr/lib64/cloud/common/scripts/vm/hypervisor/xenserver/make_migratable.sh
											</div>
										</td><td align="left">
											<div class="para">
												/opt/xensource/bin/make_migratable.sh
											</div>
										</td></tr><tr><td align="left">
											<div class="para">
												/usr/lib64/cloud/common/scripts/vm/hypervisor/xenserver/cloud-clean-vlan.sh
											</div>
										</td><td align="left">
											<div class="para">
												/opt/xensource/bin/cloud-clean-vlan.sh
											</div>
										</td></tr></tbody></table></div></li><li class="listitem"><div class="para">
							Run the following script:
						</div><pre class="programlisting"># /opt/xensource/bin/setupxenserver.sh</pre><div class="para">
							Troubleshooting: If you see the following error message, you can safely ignore it.
						</div><pre class="programlisting">mv: cannot stat `/etc/cron.daily/logrotate': No such file or directory</pre></li><li class="listitem"><div class="para">
							Plug in the storage repositories (physical block devices) to the XenServer host:
						</div><pre class="programlisting"># for pbd in `xe pbd-list currently-attached=false| grep ^uuid | awk '{print $NF}'`; do xe pbd-plug uuid=$pbd ; done</pre><div class="para">
							Note: If you add a host to this XenServer pool, you need to migrate all VMs on this host to other hosts, and eject this host from XenServer pool.
						</div></li></ol></div></li><li class="listitem"><div class="para">
					Repeat these steps to upgrade every host in the cluster to the same version of XenServer.
				</div></li><li class="listitem"><div class="para">
					Run the following command on one host in the XenServer cluster to clean up the host tags:
				</div><pre class="programlisting"># for host in $(xe host-list | grep ^uuid | awk '{print $NF}') ; do xe host-param-clear uuid=$host param-name=tags; done;</pre><div class="note"><div class="admonition_header"><h2>Note</h2></div><div class="admonition"><div class="para">
						When copying and pasting a command, be sure the command has pasted as a single line before executing. Some document viewers may introduce unwanted line breaks in copied text.
					</div></div></div></li><li class="listitem"><div class="para">
					Reconnect the XenServer cluster to CloudStack.
				</div><div class="orderedlist"><ol class="loweralpha"><li class="listitem"><div class="para">
							Log in to the CloudStack UI as root.
						</div></li><li class="listitem"><div class="para">
							Navigate to the XenServer cluster, and click Actions – Manage.
						</div></li><li class="listitem"><div class="para">
							Watch the status to see that all the hosts come up.
						</div></li></ol></div></li><li class="listitem"><div class="para">
					After all hosts are up, run the following on one host in the cluster:
				</div><pre class="programlisting"># /opt/xensource/bin/cloud-clean-vlan.sh</pre></li></ol></div></div></div><ul class="docnav"><li class="previous"><a accesskey="p" href="hypervisor-kvm-install-flow.html"><strong>Prev</strong>8.1. KVM Hypervisor Host Installation</a></li><li class="up"><a accesskey="u" href="#"><strong>Up</strong></a></li><li class="home"><a accesskey="h" href="index.html"><strong>Home</strong></a></li><li class="next"><a accesskey="n" href="vmware-install.html"><strong>Next</strong>8.3. VMware vSphere Installation and Configuration</a></li></ul></body></html>
